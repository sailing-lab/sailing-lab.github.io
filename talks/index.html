<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Sailing  Lab


  | talks/teaching

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="none" id="highlight_theme_light" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/talks/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://sailing-lab.github.io/">
       <span class="font-weight-bold">Sailing</span>   Lab
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/community/">
                community
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/open-source/">
                open-source
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/people/">
                people
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/sponsors/">
                sponsors
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/talks/">
                talks/teaching
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">talks/teaching</h1>
    <p class="post-description">Please see <a href="http://www.cs.cmu.edu/~epxing/" target="_blank" rel="noopener noreferrer"><u>here</u></a> for earlier talks.</p>
  </header>

  <article>
    <div class="publications">
<h1 id="teaching">teaching</h1>

<h3>Probabilistic Graphical Models</h3>
<a href="https://www.cs.cmu.edu/~epxing/Class/10708-20/" target="_blank" rel="noopener noreferrer">[Course Website]</a>
<p>Many of the problems in artificial intelligence, statistics, computer systems, computer vision, natural language processing, and computational biology, among many other fields, can be viewed as the search for a coherent global conclusion from local information. The probabilistic graphical models framework provides an unified view for this wide range of problems, enables efficient inference, decision-making and learning in problems with a very large number of attributes and huge datasets. This graduate-level course will provide you with a strong foundation for both applying graphical models to complex problems and for addressing core research topics in graphical models.</p>

<h3>Introduction to Machine Learning</h3>
<a href="https://www.cs.cmu.edu/~epxing/Class/10701-20/" target="_blank" rel="noopener noreferrer">[Course Website]</a>
<p>Machine Learning is concerned with computer programs that automatically improve their performance through experience (e.g., programs that learn to recognize human faces, recommend music and movies, and drive autonomous robots). This course covers the theory and practical algorithms for machine learning from a variety of perspectives. We cover topics such as Linear Regression, SVMs, Neural Networks, Graphical Models, Clustering, etc. Programming assignments include hands-on experiments with various learning algorithms. This course is designed to give a PhD-level student a thorough grounding in the methodologies, technologies, mathematics and algorithms currently needed by people who do research in machine learning.</p>




<h1 id="talks">talks</h1>
<ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CIAI</abbr>
    
  
  </div>

  <div id="hwang-ciai2022" class="col-sm-8">
    
      <div class="title">On the Utility of Gradient Compression in Distributed Training Systems</div>
      <div class="author">
        
      </div>

      <div class="periodical">
      
        <em>CIAI Colloquium, MBZUAI</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
      <a href="https://www.youtube.com/watch?v=gprhrinr3I4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>A rich body of prior work has highlighted the existence of communication bottlenecks in distributed training. To alleviate these bottlenecks, a long line of recent research proposes to use gradient compression methods. In this talk, Dr. Hongyi Wang (CMU) will first evaluate gradient compression methods’ efficacy and compare their scalability with optimized implementations of synchronous data-parallel SGD across more than 200 realistic distributed setups. The observation is that, surprisingly, only in six cases out of 200, do gradient compression methods provide promising speedup. He will then introduce our extensive investigation to identify the root causes of this phenomenon and present a performance model that can be used to identify the benefits of gradient compression for a variety of system setups. Finally, Dr. Hongyi will propose a list of desirable properties (along with two algorithmic instances) that a gradient compression method should satisfy, in order for it to provide significant speedup in real distributed training systems.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CMU AI Seminar</abbr>
    
  
  </div>

  <div id="baidu2021" class="col-sm-8">
    
      <div class="title">From Learning, to Meta-Learning, to "Lego-Learning” – theory, system, and applications</div>
      <div class="author">
        
      </div>

      <div class="periodical">
      
        <em>CMU AI Seminar</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
      <a href="https://youtu.be/_12g9kpL4K8?t=1" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Software systems for complex tasks - such as controlling manufacturing processes in real-time; or writing radiological case reports within a clinical workflow – are becoming increasingly sophisticated and consist of a large number of data, model, algorithm, and system elements and modules. Traditional benchmark/leaderboard-driven bespoke approaches in the Machine Learning community are not suited to meet the highly demanding industrial standards beyond algorithmic performance, such as cost-effectiveness, safety, scalability, and automatability, typically expected in production systems. In this talk, I discuss some technical issues toward addressing these challenges: 1) a theoretical framework for trustworthy and panoramic learning with all experiences; 2) optimization methods to best the effort for learning under such a principled framework; 3) compositional strategies for building production-grade ML programs from standard parts. I will present our recent work toward developing a standard model for Learning that unifies different machine learning paradigms and algorithms, then a Bayesian blackbox optimization approach to Meta Learning in the space of hyperparameters, model architectures, and system configurations, and finally principles and designs of standardized software Legos that facilitate cost-effective building, training, and tunning of practical ML pipelines and systems.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">KDD DLD</abbr>
    
  
  </div>

  <div id="kdddld2021" class="col-sm-8">
    
      <div class="title">It is time for deep learning to understand its expense bills</div>
      <div class="author">
        
      </div>

      <div class="periodical">
      
        <em><a href="https://deeplearningday.github.io/" target="_blank" rel="noopener noreferrer">KDD Deep Learning Day</a></em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
      <a href="https://youtu.be/1ziZcHRqtNU" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In the past several years, deep learning has dominated both academic and industrial R&amp;D over a wide range of applications, with two remarkable trends: 1) developing and training ever larger "all-purpose" monster models over all data possibly available, with a astounding 10,000x parameter number increase in recent 3 years; 2) developing and assembling end-to-end "white-boxes" deployments with ever larger number of component sub-models that need to be highly customized and interoperative. Progresses made to the leaderboards or featured in news headlines are highlighting metrics such as saliency of content production, accuracy on labeling, or speed of convergence, but a number of key challenges impacting the cost effectiveness of such results, and eventually the sustainability of current R&amp;D efforts in DL, are not receiving enough attention: 1) For large models, how many lines of code outside of the DL model are need to parallelize the computing over a computer cluster? (2) Which/How many hardware resources to use to train and deploy the model? (3) How to tune the model, the code, and the system to achieve optimum performance? (4) Can we automate composition, parallelization, tuning, and resource sharing between many users and jobs? In this talk, I will discuss these issues as a core focus in SysML research, and I will present some preliminary results on how to build standardizable, adaptive, and automatable system support for DL based on first principles (when available) underlying DL design and implementation.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL Meta-NLP</abbr>
    
  
  </div>

  <div id="acl2021-workshop" class="col-sm-8">
    
      <div class="title">Learning-to-learn through Model-based Optimization: HPO, NAS, and Distributed Systems</div>
      <div class="author">
        
      </div>

      <div class="periodical">
      
        <em><a href="https://meta-nlp-2021.github.io/" target="_blank" rel="noopener noreferrer">ACL 2021 workshop on Meta Learning and Its Applications to Natural Language Processing</a></em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
      <a href="https://youtu.be/j66_e7yVoAs" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In recent years we have seen rapid progress in developing modern NLP applications, by either building omni-purpose systems via training massive language models such as GPT-3 on big data, or building industrial solutions for specific real-world use cases via composition from pre-made modules. In both cases, a bottleneck developers often face is the effort required to determine the best way to train the model: such as how to tune the optimal configuration of hyper-parameters of the model(s), big or small, single or multiple; how to choose the best structure of a single large network or a pipeline of multiple model modules; or even how to dynamically pick the best learning rate and gradient-update transmission/synchronization scheme to achieve best “Goodput” of training on a cluster. This is a special area in meta-learning that concerns the question of “learning to learn”. However, many existing methods remain rather primitive, including random search, simple line or grid (or hyper-grid) search, and genetic algorithms, which suffer many limitations such as optimality, efficiency, scalability, adaptability, and ability to leverage domain knowledge. 
In this talk, we present a learning-to-learn methodology based on model-based optimization (MBO), which leverages machine learning models which take actions to gather information and provide recommendations to efficiently improve performance. This exhibits several advantages over existing alternatives: 1) provides adaptive/elastic algorithms that improve performance online; 2) we can incorporate domain knowledge into these models for improved recommendations; 3) can easily facilitate more-data-efficient automatic learning-to-learn, or Auto-ML. We show applications of Auto-ML via MBO in three main tasks: hyper-parameter tuning, neural architecture search, and Goodput optimization in distributed systems. We argue that such applications can improve productivity and performance of NLP systems across the board.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML ML4Data</abbr>
    
  
  </div>

  <div id="icmlml4data2021" class="col-sm-8">
    
      <div class="title">A Data-Centric View for Composable Natural Language Processing</div>
      <div class="author">
        
      </div>

      <div class="periodical">
      
        <em><a href="https://sites.google.com/view/ml4data" target="_blank" rel="noopener noreferrer">ICML2021 ML4data Workshop</a></em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
      <a href="https://youtu.be/JV2y4cT56YE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Empirical natural language processing (NLP) systems in application domains such as healthcare, finance, and education involve frequent manipulation of data and interoperation among multiple components, ranging from data ingestion, text retrieval, analysis, generation, and even human interactions like visualization and annotation. The diverse nature of the components in such complex systems imposes challenges to create standardized, robust and reusable components.  
In this talk, we present a data centric view of NLP operation and tooling, which bridges different style of software libraries, different user personas, and over additional infrastructures such as those for visualization and distributed training. We propose a highly universal data representation called DataPack, which builds on a flexible type-ontology that is morphable and extendable to subsume any commonly used data formats in all known (and hopefully, future) NLP tasks, yet remains invariant as a software data structure that can be passed across any NLP building blocks. Based on this abstraction, we develop Forte, a Data-Centric Framework for Composable NLP Workflows, with rich in-house processors, standardized 3rd-party API wrappers, and operation logics implemented at the right level of abstraction to facilitate rapid composition of sophisticated NLP solutions with heterogeneous components.  
By defining and leveraging appropriate abstractions of NLP data, Forte aims bridge silos and divergent efforts in NLP tool development, bring good software engineering practices into NLP development, with the goal to help NLP practitioners to build robust NLP systems more efficiently.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2025 Sailing  Lab.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
